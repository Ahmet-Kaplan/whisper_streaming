#!/usr/bin/env python3
"""
WhisperX Multilingual Support Example

Demonstrates Turkish and Arabic language support in the WhisperX backend,
showcasing enhanced transcription capabilities with language-specific configurations.
"""

import asyncio
import numpy as np
import wave
import tempfile
from pathlib import Path
from typing import Dict, Any

def create_synthetic_audio(text_content: str, duration: float = 5.0, sample_rate: int = 16000) -> np.ndarray:
    """Create synthetic audio that simulates speech patterns for different languages."""
    # Generate time array
    t = np.linspace(0, duration, int(duration * sample_rate))
    
    # Language-specific frequency patterns to simulate different speech characteristics
    if "turkish" in text_content.lower() or "tÃ¼rkÃ§e" in text_content.lower():
        # Turkish speech patterns - mid-range frequencies
        f1, f2, f3 = 250, 900, 1400  # Typical Turkish formants
        amplitude_mod = 0.4 + 0.3 * np.sin(8 * t)  # Turkish rhythm pattern
    elif "arabic" in text_content.lower() or "Ø¹Ø±Ø¨ÙŠ" in text_content.lower():
        # Arabic speech patterns - characteristic emphasis patterns
        f1, f2, f3 = 200, 850, 1600  # Typical Arabic formants
        amplitude_mod = 0.3 + 0.4 * np.sin(6 * t) * np.exp(-t/3)  # Arabic emphasis
    else:
        # Default English patterns
        f1, f2, f3 = 220, 800, 1200
        amplitude_mod = 0.4 + 0.2 * np.sin(10 * t)
    
    # Create complex waveform
    audio = (
        0.3 * np.sin(2 * np.pi * f1 * t) +
        0.2 * np.sin(2 * np.pi * f2 * t) +
        0.1 * np.sin(2 * np.pi * f3 * t)
    ) * amplitude_mod
    
    # Add some noise for realism
    noise = 0.02 * np.random.randn(len(audio))
    audio = audio + noise
    
    # Normalize
    audio = audio / np.max(np.abs(audio))
    
    return audio.astype(np.float32)

def save_audio_to_file(audio: np.ndarray, filename: str, sample_rate: int = 16000) -> str:
    """Save audio array to WAV file."""
    with wave.open(filename, 'wb') as wav_file:
        wav_file.setnchannels(1)  # Mono
        wav_file.setsampwidth(2)  # 16-bit
        wav_file.setframerate(sample_rate)
        
        # Convert to 16-bit integers
        audio_int16 = (audio * 32767 * 0.7).astype(np.int16)
        wav_file.writeframes(audio_int16.tobytes())
    
    return filename

async def demonstrate_turkish_transcription():
    """Demonstrate Turkish language transcription with WhisperX."""
    print("\nğŸ‡¹ğŸ‡· Turkish Language Transcription Demo")
    print("=" * 50)
    
    try:
        from whisper_streaming.backend import (
            WhisperXASR, WhisperXModelConfig, WhisperXTranscribeConfig,
            create_whisperx_asr
        )
        
        print("âœ… WhisperX backend imported successfully")
        
        # Create Turkish-specific configuration
        print("ğŸ”§ Configuring WhisperX for Turkish...")
        asr = create_whisperx_asr(
            model_name="base",  # Use base model for better Turkish support
            device="cpu",
            enable_diarization=True,  # Enable speaker diarization
            enable_alignment=True,
            enable_vad=True,  # Enable VAD for better Turkish speech detection
            normalize_audio=True,
            sample_rate=16000,
            language="tr"  # Set to Turkish
        )
        
        # Create transcribe configuration for Turkish
        transcribe_config = WhisperXTranscribeConfig(
            language="tr",  # Explicitly set Turkish
            task="transcribe",
            return_char_alignments=False,
            print_progress=True
        )
        
        print("âœ… Turkish configuration created successfully")
        
        # Generate synthetic Turkish audio
        print("ğŸµ Generating synthetic Turkish audio...")
        turkish_audio = create_synthetic_audio("Turkish speech content", duration=4.0)
        
        # Save to temporary file
        temp_file = tempfile.NamedTemporaryFile(suffix="_turkish.wav", delete=False)
        audio_file = save_audio_to_file(turkish_audio, temp_file.name)
        
        print(f"ğŸ”„ Transcribing Turkish audio: {audio_file}")
        print("   (Note: This is synthetic audio for demonstration)")
        
        # Transcribe with Turkish settings
        result = await asr.transcribe_file(audio_file, transcribe_config)
        
        # Display results
        print(f"\nâœ… Turkish Transcription Results:")
        print(f"   ğŸ—£ï¸  Detected Language: {result.language}")
        print(f"   ğŸ“ Text: {result.text or 'No speech detected (synthetic audio)'}")
        print(f"   â±ï¸  Processing Time: {getattr(result, 'processing_time', 0.0):.2f}s")
        print(f"   ğŸ“Š Segments: {len(result.segments)}")
        
        # Show model capabilities for Turkish
        model_info = asr.get_model_info()
        turkish_supported = "tr" in model_info.get("supported_languages", {})
        print(f"   ğŸŒ Turkish Support: {'âœ… Available' if turkish_supported else 'âŒ Not available'}")
        
        if model_info.get("features", {}).get("speaker_diarization"):
            print(f"   ğŸ‘¥ Speaker Diarization: âœ… Enabled")
        
        if model_info.get("features", {}).get("voice_activity_detection"):
            print(f"   ğŸ¤ VAD: âœ… Enabled")
        
        # Clean up
        Path(audio_file).unlink()
        
        return True
        
    except Exception as e:
        print(f"âŒ Turkish transcription demo failed: {e}")
        return False

async def demonstrate_arabic_transcription():
    """Demonstrate Arabic language transcription with WhisperX."""
    print("\nğŸ‡¸ğŸ‡¦ Arabic Language Transcription Demo")
    print("=" * 50)
    
    try:
        from whisper_streaming.backend import (
            WhisperXASR, WhisperXModelConfig, WhisperXTranscribeConfig,
            create_whisperx_asr
        )
        
        # Create Arabic-specific configuration
        print("ğŸ”§ Configuring WhisperX for Arabic...")
        asr = create_whisperx_asr(
            model_name="base",
            device="cpu", 
            enable_diarization=True,
            enable_alignment=True,
            enable_vad=True,
            normalize_audio=True,
            sample_rate=16000,
            language="ar"  # Set to Arabic
        )
        
        # Create transcribe configuration for Arabic
        transcribe_config = WhisperXTranscribeConfig(
            language="ar",  # Explicitly set Arabic
            task="transcribe",
            return_char_alignments=False,
            print_progress=True
        )
        
        print("âœ… Arabic configuration created successfully")
        
        # Generate synthetic Arabic audio
        print("ğŸµ Generating synthetic Arabic audio...")
        arabic_audio = create_synthetic_audio("Arabic speech content Ø¹Ø±Ø¨ÙŠ", duration=4.0)
        
        # Save to temporary file
        temp_file = tempfile.NamedTemporaryFile(suffix="_arabic.wav", delete=False)
        audio_file = save_audio_to_file(arabic_audio, temp_file.name)
        
        print(f"ğŸ”„ Transcribing Arabic audio: {audio_file}")
        print("   (Note: This is synthetic audio for demonstration)")
        
        # Transcribe with Arabic settings
        result = await asr.transcribe_file(audio_file, transcribe_config)
        
        # Display results
        print(f"\nâœ… Arabic Transcription Results:")
        print(f"   ğŸ—£ï¸  Detected Language: {result.language}")
        print(f"   ğŸ“ Text: {result.text or 'Ù„Ù… ÙŠØªÙ… Ø§ÙƒØªØ´Ø§Ù ÙƒÙ„Ø§Ù… (ØµÙˆØª ØªØ±ÙƒÙŠØ¨ÙŠ)'}")
        print(f"   â±ï¸  Processing Time: {getattr(result, 'processing_time', 0.0):.2f}s")
        print(f"   ğŸ“Š Segments: {len(result.segments)}")
        
        # Show model capabilities for Arabic
        model_info = asr.get_model_info()
        arabic_supported = "ar" in model_info.get("supported_languages", {})
        print(f"   ğŸŒ Arabic Support: {'âœ… Available' if arabic_supported else 'âŒ Not available'}")
        
        # Display RTL (Right-to-Left) language capabilities
        print(f"   ğŸ“š RTL Language Support: âœ… Arabic script supported")
        
        # Clean up
        Path(audio_file).unlink()
        
        return True
        
    except Exception as e:
        print(f"âŒ Arabic transcription demo failed: {e}")
        return False

async def demonstrate_language_detection():
    """Demonstrate automatic language detection capabilities."""
    print("\nğŸŒ Automatic Language Detection Demo")
    print("=" * 50)
    
    try:
        from whisper_streaming.backend import (
            WhisperXASR, WhisperXModelConfig, WhisperXTranscribeConfig,
            create_whisperx_asr
        )
        
        # Create ASR with auto-detection
        print("ğŸ”§ Configuring WhisperX for automatic language detection...")
        asr = create_whisperx_asr(
            model_name="base",
            device="cpu",
            enable_diarization=False,  # Disable for faster processing
            enable_alignment=True,
            enable_vad=True,
            language="auto"  # Auto-detect language
        )
        
        # Test different language samples
        test_cases = [
            {"name": "English", "content": "English speech", "expected": "en"},
            {"name": "Turkish", "content": "Turkish speech tÃ¼rkÃ§e", "expected": "tr"},
            {"name": "Arabic", "content": "Arabic speech Ø¹Ø±Ø¨ÙŠ", "expected": "ar"},
        ]
        
        for test_case in test_cases:
            print(f"\nğŸ§ª Testing {test_case['name']} detection...")
            
            # Create transcribe config with auto-detection
            transcribe_config = WhisperXTranscribeConfig(
                language=None,  # Auto-detect
                task="transcribe"
            )
            
            # Generate synthetic audio for the language
            audio = create_synthetic_audio(test_case["content"], duration=3.0)
            
            # Save to file
            temp_file = tempfile.NamedTemporaryFile(suffix=f"_{test_case['name'].lower()}.wav", delete=False)
            audio_file = save_audio_to_file(audio, temp_file.name)
            
            try:
                # Transcribe with auto-detection
                result = await asr.transcribe_file(audio_file, transcribe_config)
                
                detected_lang = result.language
                print(f"   ğŸ¯ Expected: {test_case['expected']}, Detected: {detected_lang}")
                print(f"   ğŸ“ Text: {result.text or 'No speech detected (synthetic audio)'}")
                
            except Exception as e:
                print(f"   âŒ Detection failed: {e}")
            finally:
                # Clean up
                Path(audio_file).unlink()
        
        return True
        
    except Exception as e:
        print(f"âŒ Language detection demo failed: {e}")
        return False

async def show_supported_languages():
    """Display all supported languages in WhisperX."""
    print("\nğŸ“š WhisperX Supported Languages")
    print("=" * 50)
    
    try:
        from whisper_streaming.backend import WhisperXASR, WhisperXModelConfig
        
        # Create a basic ASR instance to get language info
        config = WhisperXModelConfig(model_name="base", device="cpu")
        asr = WhisperXASR(config, sample_rate=16000, language="en")
        
        supported_languages = asr.get_supported_languages()
        
        print(f"ğŸ“Š Total Supported Languages: {len(supported_languages)}")
        print("\nğŸŒ Language List:")
        
        # Group languages by region for better display
        middle_eastern = ["ar", "fa", "ur", "he", "tr"]
        european = ["en", "de", "fr", "es", "it", "pt", "ru", "nl", "pl", "sv", "da", "no", "fi"]
        asian = ["zh", "ja", "ko", "hi", "th", "vi", "ms", "id"]
        
        print("\n  ğŸ›ï¸  Middle Eastern & Turkish:")
        for code in middle_eastern:
            if code in supported_languages:
                print(f"    {code}: {supported_languages[code]}")
        
        print("\n  ğŸ° European:")
        for code in european:
            if code in supported_languages:
                print(f"    {code}: {supported_languages[code]}")
        
        print("\n  ğŸ¯ Asian:")
        for code in asian:
            if code in supported_languages:
                print(f"    {code}: {supported_languages[code]}")
        
        print("\n  ğŸŒ Other Languages:")
        other_langs = [code for code in supported_languages.keys() 
                      if code not in middle_eastern + european + asian]
        for code in sorted(other_langs):
            print(f"    {code}: {supported_languages[code]}")
        
        # Highlight Turkish and Arabic
        print(f"\nğŸ¯ Featured Languages:")
        print(f"   ğŸ‡¹ğŸ‡· Turkish (tr): {supported_languages.get('tr', 'Not found')}")
        print(f"   ğŸ‡¸ğŸ‡¦ Arabic (ar): {supported_languages.get('ar', 'Not found')}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Failed to show supported languages: {e}")
        return False

async def main():
    """Run all multilingual demonstration examples."""
    print("ğŸŒ WhisperX Multilingual Support Demonstration")
    print("=" * 60)
    print("Showcasing Turkish and Arabic language support with enhanced features")
    
    # Check if WhisperX is available
    try:
        from whisper_streaming.backend import WhisperXASR
        print("âœ… WhisperX backend available")
    except ImportError:
        print("âŒ WhisperX backend not available. Please install whisper_streaming package.")
        return 1
    
    results = []
    
    # Run all demonstrations
    demos = [
        ("Supported Languages", show_supported_languages),
        ("Turkish Transcription", demonstrate_turkish_transcription),
        ("Arabic Transcription", demonstrate_arabic_transcription),
        ("Language Detection", demonstrate_language_detection),
    ]
    
    for demo_name, demo_func in demos:
        try:
            print(f"\nğŸš€ Running {demo_name} demo...")
            result = await demo_func()
            results.append((demo_name, result))
        except Exception as e:
            print(f"ğŸ’¥ {demo_name} demo crashed: {e}")
            results.append((demo_name, False))
    
    # Summary
    print("\nğŸ“Š Demo Results:")
    print("=" * 60)
    
    passed = 0
    for demo_name, result in results:
        status = "âœ… SUCCESS" if result else "âŒ FAILED"
        print(f"{status} {demo_name}")
        if result:
            passed += 1
    
    print(f"\nğŸ¯ Summary: {passed}/{len(results)} demos completed successfully")
    
    if passed == len(results):
        print("ğŸ‰ All multilingual demos completed successfully!")
        print("\nğŸ’¡ Next Steps:")
        print("   â€¢ Test with real Turkish and Arabic audio files")
        print("   â€¢ Experiment with speaker diarization in multilingual contexts")
        print("   â€¢ Try mixed-language transcription scenarios")
        print("   â€¢ Explore translation capabilities (transcribe â†’ translate)")
        return 0
    else:
        print("âš ï¸  Some demos failed. This may be due to missing dependencies.")
        print("   Install WhisperX: pip install whisperx")
        return 1

if __name__ == "__main__":
    import sys
    sys.exit(asyncio.run(main()))
